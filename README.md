# Fake_News_Detection_Nikesh
ğŸ“° Fake News Detection Using NLP and Machine Learning
________________________________________
ğŸ“Œ 1. Problem Statement
With the explosion of digital news sources, fake news has become a major threat to public trust and informed decision-making. The objective of this project is to develop an NLP-based machine learning model to classify news articles as either true or fake, based on their textual content.
________________________________________
ğŸ§ª 2. Methodology (Tasks from Starter Notebook)
1. Data Preparation
â€¢	Combined True.csv and Fake.csv datasets, each containing title, text, and publication date.
â€¢	Assigned labels: 1 for true, 0 for fake.
2. Text Preprocessing
â€¢	Performed tokenization, lowercasing, punctuation and digit removal.
â€¢	Used spaCy for lemmatization and stopword removal.
3. Train-Validation Split
â€¢	Split data using an 80:30 ratio for training and validation.
4. Exploratory Data Analysis (EDA)
â€¢	Plotted distribution of character lengths before and after preprocessing.
â€¢	Generated word clouds to analyze common terms in both true and fake news.
â€¢	Displayed top 10 unigrams, bigrams, and trigrams for both fake and true news.
5. Feature Extraction
â€¢	Applied TF-IDF Vectorization to transform text into numerical format.
â€¢	Additionally, used pre-trained embeddingâ€™s with word2vec-google-news-300 from the Gensim API:
o	import gensim.downloader as api
o	word2vec_model = api.load('word2vec-google-news-300')
â€¢	These embeddingâ€™s enabled semantic similarity checks and enhanced understanding of context beyond surface-level token frequency.
6. Model Training and Evaluation
â€¢	Trained and tested Random Forest, Logistic Regression, and Decision Tree classifiers.
â€¢	Chose Random Forest as the best model based on evaluation metrics.
________________________________________
ğŸ“Š 3. Visualizations
ğŸ¢ Distribution of Character Lengths â€“ Cleaned Text
 
Cleaned Text Histogram
âš™ï¸ Distribution of Character Lengths â€“ Lemmatized Text
 
Lemmatized Text Histogram
âœ… Word Cloud â€“ True News
 

âŒ Word Cloud â€“ Fake News

ğŸ”¹ Top 10 Unigrams in True News
ğŸ”¹ Top 10 Bigrams in True News
ğŸ”¹ Top 10 Trigrams in True News
ğŸ”¹ Top 10 Unigrams in Fake News
ğŸ”¹ Top 10 Bigrams in Fake News
ğŸ”¹ Top 10 Trigrams in Fake News
 

________________________________________
âš™ï¸ 4. Techniques Used
â€¢	Preprocessing: spaCy for lemmatization and stopword removal
â€¢	TF-IDF Vectorization: Converted text to weighted word vectors
â€¢	Word Embeddingâ€™s: Pre-trained Word2Vec (word2vec-google-news-300) for semantic vectorization
â€¢	Classification Models: Random Forest (best), Logistic Regression, Decision Tree
â€¢	Metrics: Accuracy, Precision, Recall, F1 Score, Confusion Matrix
________________________________________
ğŸ† 5. Model Results
â€¢	Best Model: âœ… Random Forest
â€¢	Accuracy: 0.9003
â€¢	Precision: 0.9082
â€¢	Recall: 0.8817
â€¢	F1 Score: 0.8947
F1 Score was prioritized to balance false positives and false negatives.
________________________________________
ğŸ” 6. Insights & Conclusion
â€¢	True news used structured language tied to real events; fake news favored sensational tone.
â€¢	Semantic classification via TF-IDF and Word2Vec helped reveal deeper contextual meaning.
â€¢	Random Forest achieved nearly 90% accuracy and generalizable results.
â€¢	Effective for real-world tools in news filtering, fact-checking, and misinformation detection.

